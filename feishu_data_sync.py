#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é£ä¹¦å¤šç»´è¡¨æ ¼æ•°æ®åŒæ­¥å·¥å…·
æ”¯æŒä»Excel/CSVæ–‡ä»¶è¯»å–æ•°æ®å¹¶åŒæ­¥åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼
"""

import json
import logging
import os
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any

import pandas as pd
import requests
from tqdm import tqdm


class FeishuDataSync:
    """é£ä¹¦æ•°æ®åŒæ­¥ç±»"""
    
    def __init__(self, config_path: str = "feishu_sync_config.json"):
        """
        åˆå§‹åŒ–é£ä¹¦æ•°æ®åŒæ­¥å·¥å…·
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        self.access_token = None
        self.logger = self._setup_logging()
        
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
        except json.JSONDecodeError as e:
            raise ValueError(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
    
    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        logger = logging.getLogger('FeishuDataSync')
        logger.setLevel(getattr(logging, self.config['logging']['level']))
        
        # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
        logger.handlers.clear()
        
        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # æ§åˆ¶å°å¤„ç†å™¨
        if self.config['logging']['console']:
            console_handler = logging.StreamHandler()
            console_handler.setFormatter(formatter)
            logger.addHandler(console_handler)
        
        # æ–‡ä»¶å¤„ç†å™¨
        if self.config['logging']['file']:
            log_file = Path(self.config['logging']['file'])
            log_file.parent.mkdir(parents=True, exist_ok=True)
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
        
        return logger
    
    def get_access_token(self) -> str:
        """è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œ"""
        if self.access_token:
            return self.access_token
            
        url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
        headers = {"Content-Type": "application/json"}
        data = {
            "app_id": self.config['feishu']['app_id'],
            "app_secret": self.config['feishu']['app_secret']
        }
        
        try:
            response = requests.post(url, headers=headers, json=data)
            result = response.json()
            
            if result.get("code") == 0:
                self.access_token = result.get("tenant_access_token")
                self.logger.info("âœ… æˆåŠŸè·å–è®¿é—®ä»¤ç‰Œ")
                return self.access_token
            else:
                raise Exception(f"è·å–ä»¤ç‰Œå¤±è´¥: {result}")
                
        except Exception as e:
            self.logger.error(f"âŒ è·å–è®¿é—®ä»¤ç‰Œå¤±è´¥: {e}")
            raise
    
    def load_data_source(self) -> pd.DataFrame:
        """åŠ è½½æ•°æ®æº"""
        data_config = self.config['data_source']
        file_path = data_config['file_path']
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        
        try:
            if data_config['type'] == 'excel':
                df = pd.read_excel(
                    file_path,
                    sheet_name=data_config.get('sheet_name')
                )
            elif data_config['type'] == 'csv':
                df = pd.read_csv(
                    file_path,
                    encoding=data_config.get('encoding', 'utf-8')
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {data_config['type']}")
            
            self.logger.info(f"âœ… æˆåŠŸåŠ è½½æ•°æ®æº: {file_path}")
            self.logger.info(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape}")
            self.logger.info(f"ğŸ“‹ åˆ—å: {df.columns.tolist()}")
            
            return df
            
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½æ•°æ®æºå¤±è´¥: {e}")
            raise
    
    def validate_field_mapping(self, df: pd.DataFrame) -> None:
        """éªŒè¯å­—æ®µæ˜ å°„é…ç½®"""
        self.logger.info("ğŸ” éªŒè¯å­—æ®µæ˜ å°„é…ç½®...")
        
        # æ£€æŸ¥Excelä¸­çš„æºå­—æ®µæ˜¯å¦å­˜åœ¨
        missing_source_fields = []
        for source_field in self.config['field_mapping'].keys():
            if source_field not in df.columns:
                missing_source_fields.append(source_field)
        
        if missing_source_fields:
            error_msg = f"Excelæ–‡ä»¶ä¸­ç¼ºå°‘ä»¥ä¸‹å­—æ®µ: {missing_source_fields}"
            self.logger.error(f"âŒ {error_msg}")
            raise ValueError(error_msg)
        
        # è·å–é£ä¹¦è¡¨æ ¼å­—æ®µä¿¡æ¯è¿›è¡ŒéªŒè¯
        try:
            token = self.get_access_token()
            fields_url = f"{self.config['feishu']['base_url']}/bitable/v1/apps/{self.config['feishu']['app_token']}/tables/{self.config['feishu']['table_id']}/fields"
            headers = {'Authorization': f'Bearer {token}'}
            response = requests.get(fields_url, headers=headers)
            
            if response.status_code == 200:
                feishu_fields = {field['field_name'] for field in response.json()['data']['items']}
                missing_target_fields = []
                
                for target_field in self.config['field_mapping'].values():
                    if target_field not in feishu_fields:
                        missing_target_fields.append(target_field)
                
                if missing_target_fields:
                    error_msg = f"é£ä¹¦è¡¨æ ¼ä¸­ç¼ºå°‘ä»¥ä¸‹å­—æ®µ: {missing_target_fields}"
                    self.logger.error(f"âŒ {error_msg}")
                    raise ValueError(error_msg)
                    
                self.logger.info("âœ… å­—æ®µæ˜ å°„éªŒè¯é€šè¿‡")
            else:
                self.logger.warning("âš ï¸ æ— æ³•éªŒè¯é£ä¹¦è¡¨æ ¼å­—æ®µï¼Œè·³è¿‡éªŒè¯")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ å­—æ®µéªŒè¯å¤±è´¥ï¼Œè·³è¿‡éªŒè¯: {str(e)}")
            
        self.logger.info("âœ… å­—æ®µæ˜ å°„é…ç½®éªŒè¯å®Œæˆ")
    
    def prepare_records(self, df: pd.DataFrame) -> List[Dict]:
        """å‡†å¤‡è¦æ’å…¥çš„è®°å½•æ•°æ®"""
        self.logger.info("ğŸ“ å‡†å¤‡è®°å½•æ•°æ®...")
        
        records = []
        invalid_records = 0
        
        for index, row in df.iterrows():
            try:
                fields = {}
                
                # éå†å­—æ®µæ˜ å°„é…ç½®
                for source_field, target_field in self.config['field_mapping'].items():
                    if source_field in row and pd.notna(row[source_field]):
                        value = row[source_field]
                        
                        # æ•°æ®ç±»å‹éªŒè¯å’Œè½¬æ¢ - æ›´ä¸¥æ ¼çš„NaNæ£€æŸ¥
                        if pd.isna(value) or value == '' or str(value).lower() == 'nan' or value is None:
                            continue
                        
                        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ¸…ç†
                        value_str = str(value).strip()
                        
                        # å†æ¬¡æ£€æŸ¥è½¬æ¢åçš„å­—ç¬¦ä¸²æ˜¯å¦ä¸ºç©ºæˆ–NaN
                        if not value_str or value_str.lower() == 'nan':
                            continue
                            
                        # æ ¹æ®å­—æ®µç±»å‹å¤„ç†æ•°æ®
                        if target_field in ['åœ°ç‚¹', 'æ‰€å±è¡Œä¸š', 'æ‹›è˜ç±»å‹', 'æ‹›è˜å¯¹è±¡', 'å²—ä½']:
                            # å¤šé€‰å­—æ®µ
                            if ',' in value_str:
                                fields[target_field] = [item.strip() for item in value_str.split(',') if item.strip()]
                            else:
                                fields[target_field] = [value_str] if value_str else []
                        elif target_field in ['å…¬å¸ç±»å‹', 'æ˜¯å¦ç¬”è¯•']:
                            # å•é€‰å­—æ®µï¼Œç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²
                            fields[target_field] = value_str
                        elif target_field == 'æ›´æ–°æ—¶é—´':
                            # æ—¥æœŸå­—æ®µï¼Œè½¬æ¢ä¸ºUnixæ—¶é—´æˆ³
                            try:
                                if value_str:
                                    # å°è¯•è§£ææ—¥æœŸå­—ç¬¦ä¸²
                                    dt = datetime.strptime(value_str, '%Y-%m-%d')
                                    # è½¬æ¢ä¸ºUnixæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
                                    fields[target_field] = int(dt.timestamp() * 1000)
                                else:
                                    fields[target_field] = None
                            except ValueError:
                                self.logger.warning(f"ç¬¬{index+1}è¡Œ: æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {value_str}")
                                fields[target_field] = None
                        else:
                            # æ–‡æœ¬ã€URLç­‰å…¶ä»–å­—æ®µ
                            # éªŒè¯URLå­—æ®µæ ¼å¼
                            if target_field in ['æŠ•é€’é“¾æ¥', 'å…¬å‘Šé“¾æ¥']:
                                if value_str:
                                    # å¦‚æœURLä¸åŒ…å«åè®®ï¼Œè‡ªåŠ¨æ·»åŠ https://
                                    if not (value_str.startswith('http://') or value_str.startswith('https://')):
                                        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„åŸŸåæ ¼å¼
                                        if '.' in value_str and not value_str.startswith('www.'):
                                            value_str = 'https://' + value_str
                                        elif value_str.startswith('www.'):
                                            value_str = 'https://' + value_str
                                        else:
                                            self.logger.warning(f"ç¬¬{index+1}è¡Œ: URLæ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡: {value_str}")
                                            continue
                                    
                                    # é£ä¹¦URLå­—æ®µéœ€è¦ç‰¹æ®Šæ ¼å¼ï¼šå¯¹è±¡å½¢å¼
                                    fields[target_field] = {
                                        "link": value_str,
                                        "text": value_str  # æ˜¾ç¤ºæ–‡æœ¬ï¼Œå¯ä»¥è‡ªå®šä¹‰
                                    }
                                else:
                                    continue  # ç©ºURLè·³è¿‡
                            else:
                                # æ™®é€šæ–‡æœ¬å­—æ®µ
                                fields[target_field] = value_str
                
                # åªæœ‰å½“æœ‰æœ‰æ•ˆå­—æ®µæ—¶æ‰æ·»åŠ è®°å½•
                if fields:
                    records.append({"fields": fields})
                else:
                    invalid_records += 1
                    self.logger.warning(f"ç¬¬{index+1}è¡Œ: æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
                    
            except Exception as e:
                invalid_records += 1
                self.logger.error(f"ç¬¬{index+1}è¡Œ: å¤„ç†è®°å½•æ—¶å‡ºé”™: {str(e)}")
                continue
        
        self.logger.info(f"âœ… è®°å½•å‡†å¤‡å®Œæˆ: {len(records)} æ¡æœ‰æ•ˆè®°å½•")
        if invalid_records > 0:
            self.logger.warning(f"âš ï¸ è·³è¿‡ {invalid_records} æ¡æ— æ•ˆè®°å½•")
            
        return records
    
    def batch_insert_records(self, records: List[Dict]) -> List[str]:
        """æ‰¹é‡æ’å…¥è®°å½•åˆ°é£ä¹¦"""
        if not records:
            self.logger.warning("âš ï¸  æ²¡æœ‰è®°å½•éœ€è¦æ’å…¥")
            return []
        
        token = self.get_access_token()
        batch_size = self.config['sync_options']['batch_size']
        max_retries = self.config['sync_options']['max_retries']
        retry_delay = self.config['sync_options']['retry_delay']
        
        url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{self.config['feishu']['base_id']}/tables/{self.config['feishu']['table_id']}/records/batch_create"
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
        
        all_record_ids = []
        total_batches = (len(records) + batch_size - 1) // batch_size
        
        self.logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡æ’å…¥ï¼Œæ€»è®¡ {len(records)} æ¡è®°å½•ï¼Œåˆ† {total_batches} æ‰¹å¤„ç†")
        
        with tqdm(total=len(records), desc="åŒæ­¥è¿›åº¦", unit="æ¡") as pbar:
            for i in range(0, len(records), batch_size):
                batch_records = records[i:i + batch_size]
                batch_num = i // batch_size + 1
                
                data = {"records": batch_records}
                
                # é‡è¯•æœºåˆ¶
                for attempt in range(max_retries):
                    try:
                        response = requests.post(url, headers=headers, json=data)
                        result = response.json()
                        
                        if result.get("code") == 0:
                            batch_ids = [record["record_id"] for record in result["data"]["records"]]
                            all_record_ids.extend(batch_ids)
                            
                            self.logger.info(f"âœ… æ‰¹æ¬¡ {batch_num}/{total_batches} æˆåŠŸæ’å…¥ {len(batch_ids)} æ¡è®°å½•")
                            pbar.update(len(batch_records))
                            break
                        else:
                            error_code = result.get("code", "æœªçŸ¥")
                            error_msg = result.get("msg", "æœªçŸ¥é”™è¯¯")
                            error_detail = f"æ‰¹æ¬¡ {batch_num} æ’å…¥å¤±è´¥ - é”™è¯¯ç : {error_code}, é”™è¯¯ä¿¡æ¯: {error_msg}"
                            
                            # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                            self.logger.error(f"âŒ APIå“åº”è¯¦æƒ…: {json.dumps(result, indent=2, ensure_ascii=False)}")
                            
                            if attempt < max_retries - 1:
                                self.logger.warning(f"âš ï¸  {error_detail}ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                                time.sleep(retry_delay)
                            else:
                                self.logger.error(f"âŒ {error_detail}ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")
                                
                                # å¦‚æœæ˜¯å­—æ®µéªŒè¯é”™è¯¯ï¼Œè®°å½•å…·ä½“çš„è®°å½•å†…å®¹
                                if "Invalid request parameter" in error_msg or "å­—æ®µ" in error_msg:
                                    self.logger.error(f"âŒ é—®é¢˜è®°å½•å†…å®¹: {json.dumps(batch_records, indent=2, ensure_ascii=False)}")
                                
                                raise Exception(error_detail)
                                
                    except Exception as e:
                        if attempt < max_retries - 1:
                            self.logger.warning(f"âš ï¸  æ‰¹æ¬¡ {batch_num} è¯·æ±‚å¼‚å¸¸: {e}ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                            time.sleep(retry_delay)
                        else:
                            self.logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} è¯·æ±‚å¤±è´¥: {e}")
                            raise
                
                # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼Œé¿å…APIé™æµ
                if i + batch_size < len(records):
                    time.sleep(0.5)
        
        self.logger.info(f"ğŸ‰ æ‰¹é‡æ’å…¥å®Œæˆï¼æ€»è®¡æˆåŠŸæ’å…¥ {len(all_record_ids)} æ¡è®°å½•")
        return all_record_ids
    
    def sync_data(self) -> Dict[str, Any]:
        """æ‰§è¡Œæ•°æ®åŒæ­¥"""
        start_time = datetime.now()
        self.logger.info("ğŸš€ å¼€å§‹æ•°æ®åŒæ­¥ä»»åŠ¡")
        
        try:
            # 1. åŠ è½½æ•°æ®æº
            df = self.load_data_source()
            
            # 2. éªŒè¯å­—æ®µæ˜ å°„
            self.validate_field_mapping(df)
            
            # 3. å‡†å¤‡è®°å½•
            records = self.prepare_records(df)
            
            # 4. æ‰¹é‡æ’å…¥
            record_ids = self.batch_insert_records(records)
            
            # 5. ç»Ÿè®¡ç»“æœ
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            result = {
                "success": True,
                "total_records": len(df),
                "inserted_records": len(record_ids),
                "record_ids": record_ids,
                "duration_seconds": duration,
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat()
            }
            
            self.logger.info(f"ğŸ‰ æ•°æ®åŒæ­¥ä»»åŠ¡å®Œæˆï¼")
            self.logger.info(f"ğŸ“Š æ€»è®°å½•æ•°: {result['total_records']}")
            self.logger.info(f"âœ… æˆåŠŸæ’å…¥: {result['inserted_records']}")
            self.logger.info(f"â±ï¸  è€—æ—¶: {duration:.2f} ç§’")
            
            return result
            
        except Exception as e:
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            result = {
                "success": False,
                "error": str(e),
                "duration_seconds": duration,
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat()
            }
            
            self.logger.error(f"âŒ æ•°æ®åŒæ­¥ä»»åŠ¡å¤±è´¥: {e}")
            return result


def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºåŒæ­¥å™¨å®ä¾‹
        syncer = FeishuDataSync()
        
        # æ‰§è¡ŒåŒæ­¥
        result = syncer.sync_data()
        
        # è¾“å‡ºç»“æœ
        if result['success']:
            print(f"\nğŸ‰ åŒæ­¥æˆåŠŸå®Œæˆï¼")
            print(f"ğŸ“Š æ’å…¥è®°å½•æ•°: {result['inserted_records']}")
            print(f"â±ï¸  è€—æ—¶: {result['duration_seconds']:.2f} ç§’")
        else:
            print(f"\nâŒ åŒæ­¥å¤±è´¥: {result['error']}")
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")


if __name__ == "__main__":
    main()