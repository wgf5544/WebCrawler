#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çˆ¬è™« - ç®€åŒ–ç‰ˆOCRåŠŸèƒ½
ä½¿ç”¨Tesseract OCRè¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—

ä¾èµ–å®‰è£…:
pip install selenium beautifulsoup4 lxml pandas pillow pytesseract
brew install tesseract tesseract-lang  # macOS
"""

import time
import requests
import traceback
from io import BytesIO
from urllib.parse import urljoin, urlparse
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
from PIL import Image
import pandas as pd

# OCRå¼•æ“
try:
    import pytesseract
    TESSERACT_AVAILABLE = True
    print("âœ… Tesseract OCRå¯ç”¨")
except ImportError:
    TESSERACT_AVAILABLE = False
    print("âŒ Tesseract OCRæœªå®‰è£…")


class WeixinOCRCrawler:
    """å¾®ä¿¡æ–‡ç« çˆ¬è™« - ç®€åŒ–ç‰ˆOCRåŠŸèƒ½"""
    
    def __init__(self, headless=True, wait_time=15):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            wait_time: é¡µé¢åŠ è½½ç­‰å¾…æ—¶é—´
        """
        self.headless = headless
        self.wait_time = wait_time
        self.driver = None
    
    def setup_driver(self):
        """è®¾ç½®Chromeæµè§ˆå™¨é©±åŠ¨"""
        chrome_options = Options()
        
        if self.headless:
            chrome_options.add_argument("--headless=new")
        
        # åçˆ¬è™«æ£€æµ‹é…ç½®
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # è®¾ç½®User-Agent
        chrome_options.add_argument(
            "--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        
        self.driver = webdriver.Chrome(options=chrome_options)
        
        # æ‰§è¡Œåæ£€æµ‹è„šæœ¬
        self.driver.execute_script(
            "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
        )
        
        print("âœ… æµè§ˆå™¨é©±åŠ¨åˆå§‹åŒ–å®Œæˆ")
    
    def crawl_article(self, url):
        """çˆ¬å–å¾®ä¿¡æ–‡ç« é¡µé¢æºä»£ç """
        if not self.driver:
            self.setup_driver()
        
        try:
            print(f"æ­£åœ¨è®¿é—®: {url}")
            self.driver.get(url)
            
            # æ£€æŸ¥é¡µé¢çŠ¶æ€
            if "å‚æ•°é”™è¯¯" in self.driver.page_source or "è¯¥å†…å®¹å·²è¢«å‘å¸ƒè€…åˆ é™¤" in self.driver.page_source:
                print("âš ï¸  æ£€æµ‹åˆ°å‚æ•°é”™è¯¯ï¼ŒURLå¯èƒ½æ— æ•ˆ")
            
            # ç­‰å¾…é¡µé¢åŸºæœ¬åŠ è½½
            time.sleep(3)
            
            # ç­‰å¾…æ ‡é¢˜åŠ è½½
            wait = WebDriverWait(self.driver, self.wait_time)
            try:
                wait.until(EC.any_of(
                    EC.presence_of_element_located((By.ID, "activity-name")),
                    EC.presence_of_element_located((By.CLASS_NAME, "rich_media_title")),
                    EC.presence_of_element_located((By.TAG_NAME, "h1"))
                ))
                print("âœ… æ ‡é¢˜å·²åŠ è½½")
            except TimeoutException:
                print("âš ï¸  æ ‡é¢˜åŠ è½½è¶…æ—¶")
            
            # ç­‰å¾…å†…å®¹åŒºåŸŸåŠ è½½
            try:
                wait.until(EC.any_of(
                    EC.presence_of_element_located((By.ID, "js_content")),
                    EC.presence_of_element_located((By.CLASS_NAME, "rich_media_content")),
                    EC.presence_of_element_located((By.TAG_NAME, "article"))
                ))
                print("âœ… å†…å®¹åŒºåŸŸå·²åŠ è½½")
            except TimeoutException:
                print("âš ï¸  å†…å®¹åŒºåŸŸåŠ è½½è¶…æ—¶")
            
            # æ»šåŠ¨é¡µé¢è§¦å‘æ‡’åŠ è½½
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(1)
            
            # è§¦å‘å¯èƒ½çš„åŠ¨æ€å†…å®¹åŠ è½½
            self.driver.execute_script("""
                // è§¦å‘å„ç§å¯èƒ½çš„äº‹ä»¶
                window.dispatchEvent(new Event('scroll'));
                window.dispatchEvent(new Event('resize'));
                document.dispatchEvent(new Event('DOMContentLoaded'));
            """)
            
            time.sleep(3)
            
            page_source = self.driver.page_source
            print(f"é¡µé¢æºä»£ç é•¿åº¦: {len(page_source)}")
            
            return page_source
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
            traceback.print_exc()
            return None
    
    def download_image(self, img_url, base_url):
        """ä¸‹è½½å›¾ç‰‡"""
        try:
            # å¤„ç†ç›¸å¯¹URL
            if img_url.startswith('//'):
                img_url = 'https:' + img_url
            elif img_url.startswith('/'):
                img_url = urljoin(base_url, img_url)
            
            # è·³è¿‡base64å›¾ç‰‡å’Œæ— æ•ˆURL
            if img_url.startswith('data:') or not img_url.startswith('http'):
                return None
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Referer': base_url
            }
            
            response = requests.get(img_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            # è½¬æ¢ä¸ºPIL Imageå¯¹è±¡
            image = Image.open(BytesIO(response.content))
            
            # è½¬æ¢ä¸ºRGBæ¨¡å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            return image
            
        except Exception as e:
            print(f"âš ï¸  å›¾ç‰‡ä¸‹è½½å¤±è´¥ {img_url}: {e}")
            return None
    
    def extract_text_from_image(self, image):
        """ä½¿ç”¨Tesseract OCRä»å›¾ç‰‡ä¸­æå–æ–‡å­—"""
        if not image or not TESSERACT_AVAILABLE:
            return ""
        
        try:
            # ä½¿ç”¨Tesseract OCRï¼Œæ”¯æŒä¸­è‹±æ–‡
            # é…ç½®OCRå‚æ•°
            custom_config = r'--oem 3 --psm 6 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡äº¿é›¶å£¹è´°åè‚†ä¼é™†æŸ’æŒç–æ‹¾ä½°ä»Ÿè¬å„„é›¶'
            
            # å…ˆå°è¯•ä¸­è‹±æ–‡æ··åˆè¯†åˆ«
            text = pytesseract.image_to_string(image, lang='chi_sim+eng', config=custom_config)
            
            # å¦‚æœç»“æœä¸ºç©ºï¼Œå°è¯•åªç”¨è‹±æ–‡
            if not text.strip():
                text = pytesseract.image_to_string(image, lang='eng')
            
            # å¦‚æœè¿˜æ˜¯ä¸ºç©ºï¼Œå°è¯•åªç”¨ä¸­æ–‡
            if not text.strip():
                text = pytesseract.image_to_string(image, lang='chi_sim')
            
            # æ¸…ç†æ–‡æœ¬
            text = text.strip()
            # ç§»é™¤è¿‡çŸ­çš„è¯†åˆ«ç»“æœï¼ˆå¯èƒ½æ˜¯å™ªå£°ï¼‰
            if len(text) < 3:
                return ""
            
            return text
            
        except Exception as e:
            print(f"âš ï¸  OCRè¯†åˆ«å¤±è´¥: {e}")
            return ""
    
    def extract_content_with_ocr(self, page_source, base_url=None):
        """æå–æ–‡ç« å†…å®¹ï¼ŒåŒ…æ‹¬OCRå›¾ç‰‡æ–‡å­—è¯†åˆ«"""
        if not page_source:
            return {
                'title': 'æœªçŸ¥æ ‡é¢˜',
                'content': '',
                'images': [],
                'image_texts': [],
                'tables': []
            }
        
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = "æœªçŸ¥æ ‡é¢˜"
        title_selectors = [
            "#activity-name",
            ".rich_media_title",
            "h1",
            ".title",
            "title"
        ]
        
        for selector in title_selectors:
            title_element = soup.select_one(selector)
            if title_element and title_element.get_text(strip=True):
                title = title_element.get_text(strip=True)
                print(f"âœ… æ‰¾åˆ°æ ‡é¢˜: {title}")
                break
        
        if title == "æœªçŸ¥æ ‡é¢˜":
            print("âš ï¸  æœªæ‰¾åˆ°æœ‰æ•ˆæ ‡é¢˜")
        
        # æå–å†…å®¹
        content_selectors = [
            "#js_content",
            ".rich_media_content",
            "article",
            ".article-content",
            "main",
            ".content"
        ]
        
        content_element = None
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                content_element = element
                print(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨ {selector} æå–åˆ°å†…å®¹")
                break
        
        if not content_element:
            print("âš ï¸  æœªæ‰¾åˆ°å†…å®¹åŒºåŸŸï¼Œå°è¯•ä»bodyæå–")
            content_element = soup.find('body')
        
        # æå–æ–‡æœ¬å†…å®¹å’Œå›¾ç‰‡
        paragraphs = []
        images = []
        image_texts = []
        
        if content_element:
            # æå–å›¾ç‰‡ä¿¡æ¯å’ŒOCRæ–‡å­—
            img_elements = content_element.find_all('img')
            print(f"ğŸ–¼ï¸  æ‰¾åˆ° {len(img_elements)} å¼ å›¾ç‰‡")
            
            for i, img in enumerate(img_elements, 1):
                img_src = img.get('src', '')
                img_alt = img.get('alt', '')
                img_title = img.get('title', '')
                
                img_info = {
                    'index': i,
                    'src': img_src,
                    'alt': img_alt,
                    'title': img_title
                }
                images.append(img_info)
                
                # å¦‚æœaltå±æ€§æœ‰æœ‰æ„ä¹‰çš„æ–‡å­—ï¼Œå…ˆä½¿ç”¨alt
                if img_alt and len(img_alt) > 2 and img_alt not in ['å›¾ç‰‡', 'å›¾', 'image']:
                    image_texts.append({
                        'image_index': i,
                        'text': img_alt,
                        'source': 'alt_attribute',
                        'src': img_src
                    })
                    paragraphs.append(f"[å›¾ç‰‡{i}]: {img_alt}")
                    print(f"âœ… å›¾ç‰‡ {i} ä½¿ç”¨altå±æ€§: {img_alt}")
                    continue
                
                # OCRè¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—
                if TESSERACT_AVAILABLE and img_src:
                    print(f"ğŸ” æ­£åœ¨OCRè¯†åˆ«ç¬¬ {i} å¼ å›¾ç‰‡...")
                    image_obj = self.download_image(img_src, base_url or '')
                    if image_obj:
                        ocr_text = self.extract_text_from_image(image_obj)
                        if ocr_text:
                            image_texts.append({
                                'image_index': i,
                                'text': ocr_text,
                                'source': 'ocr',
                                'src': img_src
                            })
                            paragraphs.append(f"[å›¾ç‰‡{i}æ–‡å­—å†…å®¹]: {ocr_text}")
                            print(f"âœ… å›¾ç‰‡ {i} OCRè¯†åˆ«: {ocr_text[:50]}...")
                        else:
                            print(f"âš ï¸  å›¾ç‰‡ {i} OCRæœªè¯†åˆ«åˆ°æ–‡å­—")
                    else:
                        print(f"âš ï¸  å›¾ç‰‡ {i} ä¸‹è½½å¤±è´¥")
            
            # æå–å¸¸è§„æ–‡æœ¬å†…å®¹
            text_elements = content_element.find_all(['p', 'div', 'span', 'section', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            
            for element in text_elements:
                text = element.get_text(strip=True)
                if text and len(text) > 5 and not self._is_navigation_text(text):
                    paragraphs.append(text)
            
            # å¦‚æœæ²¡æœ‰æå–åˆ°è¶³å¤Ÿçš„æ–‡æœ¬ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            if len([p for p in paragraphs if not p.startswith('[å›¾ç‰‡')]) < 2:
                print("âš ï¸  å¸¸è§„æ–‡æœ¬è¾ƒå°‘ï¼Œå°è¯•æå–æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹")
                for string in content_element.stripped_strings:
                    text = string.strip()
                    if len(text) > 5 and not self._is_navigation_text(text):
                        paragraphs.append(text)
        
        # æå–è¡¨æ ¼
        tables = []
        table_elements = soup.find_all('table')
        for table in table_elements:
            table_data = []
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                row_data = [cell.get_text(strip=True) for cell in cells]
                if any(row_data):  # åªæ·»åŠ éç©ºè¡Œ
                    table_data.append(row_data)
            if table_data:
                tables.append(table_data)
        
        # å»é‡å’Œè¿‡æ»¤
        unique_paragraphs = []
        seen = set()
        for p in paragraphs:
            if p not in seen and len(p.strip()) > 3:
                unique_paragraphs.append(p)
                seen.add(p)
        
        content = '\n\n'.join(unique_paragraphs)
        
        return {
            'title': title,
            'content': content,
            'images': images,
            'image_texts': image_texts,
            'tables': tables
        }
    
    def _is_navigation_text(self, text):
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¯¼èˆªæˆ–æ— å…³æ–‡æœ¬"""
        nav_keywords = [
            "å¾®ä¿¡æ‰«ä¸€æ‰«", "å…³æ³¨è¯¥å…¬ä¼—å·", "åˆ†äº«åˆ°", "ç‚¹å‡»æŸ¥çœ‹", "é˜…è¯»åŸæ–‡",
            "åœ¨çœ‹", "ç‚¹èµ", "åˆ†äº«", "æ”¶è—", "å†™ç•™è¨€", "æœ‹å‹åœˆ", "QQç©ºé—´",
            "æ–°æµªå¾®åš", "QQå¥½å‹", "è¯·åœ¨å¾®ä¿¡æœç´¢", "é•¿æŒ‰è¯†åˆ«", "æ‰«æäºŒç»´ç ",
            "å‚æ•°é”™è¯¯", "è¯¥å†…å®¹å·²è¢«å‘å¸ƒè€…åˆ é™¤", "æ­¤å†…å®¹å› è¿è§„æ— æ³•æŸ¥çœ‹"
        ]
        return any(keyword in text for keyword in nav_keywords)
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            self.driver.quit()
            print("âœ… æµè§ˆå™¨å·²å…³é—­")
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.close()


def test_weixin_ocr():
    """æµ‹è¯•å¾®ä¿¡çˆ¬è™«OCRåŠŸèƒ½"""
    # æµ‹è¯•URL
    test_url = "https://mp.weixin.qq.com/s/kHAv-XraEN_82NGegRlZUw"
    
    print("ğŸš€ å¾®ä¿¡æ–‡ç« çˆ¬è™« - OCRæ–‡å­—è¯†åˆ«æµ‹è¯•")
    print("=" * 60)
    print(f"æµ‹è¯•URL: {test_url}")
    print("=" * 60)
    
    try:
        with WeixinOCRCrawler(headless=True) as crawler:
            # çˆ¬å–é¡µé¢
            page_source = crawler.crawl_article(test_url)
            
            if page_source:
                # æå–å†…å®¹ï¼ˆåŒ…æ‹¬OCRï¼‰
                result = crawler.extract_content_with_ocr(page_source, test_url)
                
                print(f"\nğŸ“Š æå–ç»“æœ:")
                print(f"æ ‡é¢˜: {result['title']}")
                print(f"å†…å®¹é•¿åº¦: {len(result['content'])} å­—ç¬¦")
                print(f"å›¾ç‰‡æ•°é‡: {len(result['images'])}")
                print(f"è¯†åˆ«åˆ°æ–‡å­—çš„å›¾ç‰‡æ•°é‡: {len(result['image_texts'])}")
                print(f"è¡¨æ ¼æ•°é‡: {len(result['tables'])}")
                
                # æ˜¾ç¤ºå›¾ç‰‡ä¿¡æ¯
                if result['images']:
                    print(f"\nğŸ–¼ï¸  å›¾ç‰‡ä¿¡æ¯:")
                    for img in result['images']:
                        print(f"å›¾ç‰‡ {img['index']}: {img['alt'] or 'æ— altå±æ€§'}")
                
                # æ˜¾ç¤ºå›¾ç‰‡OCRç»“æœ
                if result['image_texts']:
                    print(f"\nğŸ” å›¾ç‰‡æ–‡å­—è¯†åˆ«ç»“æœ:")
                    for img_text in result['image_texts']:
                        source_type = "altå±æ€§" if img_text['source'] == 'alt_attribute' else "OCRè¯†åˆ«"
                        print(f"å›¾ç‰‡ {img_text['image_index']} ({source_type}): {img_text['text'][:100]}...")
                
                # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
                if result['content']:
                    print(f"\nğŸ“„ å†…å®¹é¢„è§ˆ:")
                    preview = result['content'][:300] + "..." if len(result['content']) > 300 else result['content']
                    print(preview)
                else:
                    print(f"\nâš ï¸  æœªèƒ½æå–åˆ°æ–‡æœ¬å†…å®¹")
                    print("å¯èƒ½çš„åŸå› :")
                    print("- æ–‡ç« ä¸»è¦åŒ…å«å›¾ç‰‡å†…å®¹ï¼ˆå·²é€šè¿‡OCRè¯†åˆ«ï¼‰")
                    print("- éœ€è¦ç™»å½•å¾®ä¿¡æŸ¥çœ‹")
                    print("- URLå·²å¤±æ•ˆæˆ–éœ€è¦ç‰¹æ®Šæƒé™")
                    
            else:
                print("âŒ æœªèƒ½è·å–é¡µé¢å†…å®¹")
                
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
    
    print("\nâœ… æµ‹è¯•å®Œæˆ")
    
    print("\nğŸ’¡ OCRåŠŸèƒ½ä½¿ç”¨æç¤º:")
    print("1. OCRè¯†åˆ«å‡†ç¡®ç‡å–å†³äºå›¾ç‰‡è´¨é‡å’Œæ–‡å­—æ¸…æ™°åº¦")
    print("2. æ”¯æŒä¸­è‹±æ–‡æ··åˆè¯†åˆ«")
    print("3. ä¼˜å…ˆä½¿ç”¨å›¾ç‰‡çš„altå±æ€§ï¼Œå¦‚æœæ²¡æœ‰å†è¿›è¡ŒOCRè¯†åˆ«")
    print("4. å»ºè®®åœ¨ç½‘ç»œè‰¯å¥½çš„ç¯å¢ƒä¸‹ä½¿ç”¨")
    print("5. å¦‚æœå›¾ç‰‡è¾ƒå¤šï¼Œè¯†åˆ«è¿‡ç¨‹å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´")


if __name__ == "__main__":
    test_weixin_ocr()