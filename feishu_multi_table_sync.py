#!/usr/bin/env python3
"""
é£ä¹¦å¤šç»´è¡¨æ ¼æ•°æ®åŒæ­¥å·¥å…· - å¤šè¡¨æ ¼ç‰ˆæœ¬
æ”¯æŒåŒæ—¶å‘å¤šä¸ªé£ä¹¦å¤šç»´è¡¨æ ¼åŒæ­¥æ•°æ®
"""

import json
import logging
import os
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any

import pandas as pd
import requests
from tqdm import tqdm


class FeishuMultiTableSync:
    """é£ä¹¦å¤šè¡¨æ ¼æ•°æ®åŒæ­¥å™¨"""
    
    def __init__(self, config_path: str = "feishu_multi_table_config.json"):
        """åˆå§‹åŒ–åŒæ­¥å™¨"""
        self.config = self._load_config(config_path)
        self.logger = self._setup_logging()
        self.access_token = None
        self.token_expires_at = 0
        
        self.logger.info("ğŸš€ é£ä¹¦å¤šè¡¨æ ¼æ•°æ®åŒæ­¥å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return config
        except FileNotFoundError:
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        except json.JSONDecodeError as e:
            raise ValueError(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
    
    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger("FeishuMultiTableSync")
        logger.setLevel(getattr(logging, self.config['logging']['level']))
        
        # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
        
        # æ–‡ä»¶å¤„ç†å™¨
        log_file = Path(self.config['logging']['file'])
        log_file.parent.mkdir(parents=True, exist_ok=True)
        
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(file_formatter)
        logger.addHandler(file_handler)
        
        # æ§åˆ¶å°å¤„ç†å™¨
        if self.config['logging']['console']:
            console_handler = logging.StreamHandler()
            console_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)
        
        return logger
    
    def get_access_token(self) -> str:
        """è·å–è®¿é—®ä»¤ç‰Œ"""
        # æ£€æŸ¥ä»¤ç‰Œæ˜¯å¦ä»ç„¶æœ‰æ•ˆ
        if self.access_token and time.time() < self.token_expires_at:
            return self.access_token
        
        self.logger.info("ğŸ”‘ è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œ...")
        
        url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
        headers = {"Content-Type": "application/json"}
        data = {
            "app_id": self.config['feishu']['app_id'],
            "app_secret": self.config['feishu']['app_secret']
        }
        
        response = requests.post(url, headers=headers, json=data)
        result = response.json()
        
        if result.get("code") == 0:
            self.access_token = result["tenant_access_token"]
            # è®¾ç½®ä»¤ç‰Œè¿‡æœŸæ—¶é—´ï¼ˆæå‰5åˆ†é’Ÿåˆ·æ–°ï¼‰
            self.token_expires_at = time.time() + result.get("expire", 7200) - 300
            self.logger.info("âœ… è®¿é—®ä»¤ç‰Œè·å–æˆåŠŸ")
            return self.access_token
        else:
            raise Exception(f"è·å–è®¿é—®ä»¤ç‰Œå¤±è´¥: {result}")
    
    def load_data_source(self) -> pd.DataFrame:
        """åŠ è½½æ•°æ®æº"""
        self.logger.info("ğŸ“‚ åŠ è½½æ•°æ®æº...")
        
        data_config = self.config['data_source']
        
        if data_config['type'] == 'excel':
            file_path = data_config['file_path']
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"Excelæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            # è¯»å–Excelæ–‡ä»¶
            df = pd.read_excel(
                file_path,
                sheet_name=data_config['sheet_name'],
                engine='openpyxl'
            )
            
            self.logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(df)} è¡Œæ•°æ®")
            return df
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {data_config['type']}")
    
    def validate_field_mapping(self, df: pd.DataFrame, field_mapping: Dict[str, str]) -> None:
        """éªŒè¯å­—æ®µæ˜ å°„"""
        self.logger.info("ğŸ” éªŒè¯å­—æ®µæ˜ å°„é…ç½®...")
        
        missing_fields = []
        for source_field in field_mapping.keys():
            if source_field not in df.columns:
                missing_fields.append(source_field)
                self.logger.warning(f"âš ï¸ å­—æ®µéªŒè¯å¤±è´¥ï¼Œè·³è¿‡éªŒè¯: '{source_field}'")
        
        if missing_fields:
            self.logger.warning(f"âš ï¸ ä»¥ä¸‹å­—æ®µåœ¨æ•°æ®æºä¸­ä¸å­˜åœ¨: {missing_fields}")
        
        self.logger.info("âœ… å­—æ®µæ˜ å°„é…ç½®éªŒè¯å®Œæˆ")
    
    def clean_field_value(self, field_name: str, value: str) -> str:
        """æ¸…æ´—å­—æ®µå€¼"""
        if 'field_cleaning' not in self.config:
            return value
        
        field_cleaning_rules = self.config['field_cleaning']
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¯¥å­—æ®µçš„æ¸…æ´—è§„åˆ™
        if field_name in field_cleaning_rules:
            cleaning_map = field_cleaning_rules[field_name]
            # å¦‚æœæ‰¾åˆ°åŒ¹é…çš„è§„åˆ™ï¼Œè¿”å›æ¸…æ´—åçš„å€¼
            if value in cleaning_map:
                cleaned_value = cleaning_map[value]
                self.logger.debug(f"å­—æ®µå€¼æ¸…æ´—: {field_name} '{value}' -> '{cleaned_value}'")
                return cleaned_value
        
        return value
    
    def prepare_records(self, df: pd.DataFrame, field_mapping: Dict[str, str]) -> List[Dict]:
        """å‡†å¤‡è®°å½•æ•°æ®"""
        self.logger.info("ğŸ“ å‡†å¤‡è®°å½•æ•°æ®...")
        
        records = []
        invalid_records = 0
        
        for index, row in df.iterrows():
            try:
                fields = {}
                
                for source_field, target_field in field_mapping.items():
                    if source_field not in df.columns:
                        continue
                    
                    value = row[source_field]
                    
                    # å¤„ç†ç©ºå€¼å’ŒNaN - æ›´ä¸¥æ ¼çš„æ£€æŸ¥
                    if pd.isna(value) or value == '' or str(value).lower() == 'nan' or value is None:
                        continue
                    
                    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ¸…ç†
                    value_str = str(value).strip()
                    
                    # å†æ¬¡æ£€æŸ¥è½¬æ¢åçš„å­—ç¬¦ä¸²æ˜¯å¦ä¸ºç©ºæˆ–NaN
                    if not value_str or value_str.lower() == 'nan':
                        continue
                    
                    # åº”ç”¨å­—æ®µå€¼æ¸…æ´—è§„åˆ™
                    value_str = self.clean_field_value(source_field, value_str)
                        
                    # æ ¹æ®å­—æ®µç±»å‹å¤„ç†æ•°æ®
                    if target_field in ['åœ°ç‚¹', 'æ‰€å±è¡Œä¸š', 'æ‹›è˜ç±»å‹', 'æ‹›è˜å¯¹è±¡', 'å²—ä½']:
                        # å¤šé€‰å­—æ®µ
                        if ',' in value_str:
                            # å¯¹æ¯ä¸ªåˆ†å‰²åçš„å€¼ä¹Ÿåº”ç”¨æ¸…æ´—è§„åˆ™
                            cleaned_items = []
                            for item in value_str.split(','):
                                item = item.strip()
                                if item:
                                    cleaned_item = self.clean_field_value(source_field, item)
                                    cleaned_items.append(cleaned_item)
                            fields[target_field] = cleaned_items
                        else:
                            fields[target_field] = [value_str] if value_str else []
                    elif target_field in ['å…¬å¸ç±»å‹', 'æ˜¯å¦ç¬”è¯•']:
                        # å•é€‰å­—æ®µï¼Œç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²
                        fields[target_field] = value_str
                    elif target_field == 'æ›´æ–°æ—¶é—´':
                        # æ—¥æœŸå­—æ®µï¼Œè½¬æ¢ä¸ºUnixæ—¶é—´æˆ³
                        try:
                            if value_str:
                                # å°è¯•è§£ææ—¥æœŸå­—ç¬¦ä¸²
                                dt = datetime.strptime(value_str, '%Y-%m-%d')
                                # è½¬æ¢ä¸ºUnixæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
                                fields[target_field] = int(dt.timestamp() * 1000)
                            else:
                                fields[target_field] = None
                        except ValueError:
                            self.logger.warning(f"ç¬¬{index+1}è¡Œ: æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {value_str}")
                            fields[target_field] = None
                    else:
                        # æ–‡æœ¬ã€URLç­‰å…¶ä»–å­—æ®µ
                        # éªŒè¯URLå­—æ®µæ ¼å¼
                        if target_field in ['æŠ•é€’é“¾æ¥', 'å…¬å‘Šé“¾æ¥']:
                            if value_str:
                                # å¦‚æœURLä¸åŒ…å«åè®®ï¼Œè‡ªåŠ¨æ·»åŠ https://
                                if not (value_str.startswith('http://') or value_str.startswith('https://')):
                                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„åŸŸåæ ¼å¼
                                    if '.' in value_str and not value_str.startswith('www.'):
                                        value_str = 'https://' + value_str
                                    elif value_str.startswith('www.'):
                                        value_str = 'https://' + value_str
                                    else:
                                        self.logger.warning(f"ç¬¬{index+1}è¡Œ: URLæ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡: {value_str}")
                                        continue
                                
                                # é£ä¹¦URLå­—æ®µéœ€è¦ç‰¹æ®Šæ ¼å¼ï¼šå¯¹è±¡å½¢å¼
                                fields[target_field] = {
                                    "link": value_str,
                                    "text": value_str  # æ˜¾ç¤ºæ–‡æœ¬ï¼Œå¯ä»¥è‡ªå®šä¹‰
                                }
                            else:
                                continue  # ç©ºURLè·³è¿‡
                        else:
                            # æ™®é€šæ–‡æœ¬å­—æ®µ
                            fields[target_field] = value_str
                
                # åªæœ‰å½“æœ‰æœ‰æ•ˆå­—æ®µæ—¶æ‰æ·»åŠ è®°å½•
                if fields:
                    records.append({"fields": fields})
                else:
                    invalid_records += 1
                    self.logger.warning(f"ç¬¬{index+1}è¡Œ: æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
                    
            except Exception as e:
                invalid_records += 1
                self.logger.error(f"ç¬¬{index+1}è¡Œ: å¤„ç†è®°å½•æ—¶å‡ºé”™: {str(e)}")
                continue
        
        self.logger.info(f"âœ… è®°å½•å‡†å¤‡å®Œæˆ: {len(records)} æ¡æœ‰æ•ˆè®°å½•")
        if invalid_records > 0:
            self.logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆè®°å½•: {invalid_records} æ¡")
        
        return records
    
    def batch_insert_records(self, records: List[Dict], base_id: str, table_id: str, table_name: str) -> List[str]:
        """æ‰¹é‡æ’å…¥è®°å½•åˆ°æŒ‡å®šè¡¨æ ¼"""
        if not records:
            self.logger.warning(f"âš ï¸ [{table_name}] æ²¡æœ‰è®°å½•éœ€è¦æ’å…¥")
            return []
        
        token = self.get_access_token()
        batch_size = self.config['sync_options']['batch_size']
        max_retries = self.config['sync_options']['max_retries']
        retry_delay = self.config['sync_options']['retry_delay']
        
        url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{base_id}/tables/{table_id}/records/batch_create"
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
        
        all_record_ids = []
        total_batches = (len(records) + batch_size - 1) // batch_size
        
        self.logger.info(f"ğŸš€ [{table_name}] å¼€å§‹æ‰¹é‡æ’å…¥ï¼Œæ€»è®¡ {len(records)} æ¡è®°å½•ï¼Œåˆ† {total_batches} æ‰¹å¤„ç†")
        
        with tqdm(total=len(records), desc=f"{table_name}åŒæ­¥è¿›åº¦", unit="æ¡") as pbar:
            for i in range(0, len(records), batch_size):
                batch_records = records[i:i + batch_size]
                batch_num = i // batch_size + 1
                
                data = {"records": batch_records}
                
                # é‡è¯•æœºåˆ¶
                for attempt in range(max_retries):
                    try:
                        response = requests.post(url, headers=headers, json=data)
                        result = response.json()
                        
                        if result.get("code") == 0:
                            batch_ids = [record["record_id"] for record in result["data"]["records"]]
                            all_record_ids.extend(batch_ids)
                            
                            self.logger.info(f"âœ… [{table_name}] æ‰¹æ¬¡ {batch_num}/{total_batches} æˆåŠŸæ’å…¥ {len(batch_ids)} æ¡è®°å½•")
                            pbar.update(len(batch_records))
                            break
                        else:
                            error_code = result.get("code", "æœªçŸ¥")
                            error_msg = result.get("msg", "æœªçŸ¥é”™è¯¯")
                            error_detail = f"[{table_name}] æ‰¹æ¬¡ {batch_num} æ’å…¥å¤±è´¥ - é”™è¯¯ç : {error_code}, é”™è¯¯ä¿¡æ¯: {error_msg}"
                            
                            # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                            self.logger.error(f"âŒ [{table_name}] APIå“åº”è¯¦æƒ…: {json.dumps(result, indent=2, ensure_ascii=False)}")
                            
                            if attempt < max_retries - 1:
                                self.logger.warning(f"âš ï¸ {error_detail}ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                                time.sleep(retry_delay)
                            else:
                                self.logger.error(f"âŒ {error_detail}ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")
                                
                                # å¦‚æœæ˜¯å­—æ®µéªŒè¯é”™è¯¯ï¼Œè®°å½•å…·ä½“çš„è®°å½•å†…å®¹
                                if "Invalid request parameter" in error_msg or "å­—æ®µ" in error_msg:
                                    self.logger.error(f"âŒ [{table_name}] é—®é¢˜è®°å½•å†…å®¹: {json.dumps(batch_records, indent=2, ensure_ascii=False)}")
                                
                                if not self.config['sync_options'].get('continue_on_error', False):
                                    raise Exception(error_detail)
                                else:
                                    self.logger.warning(f"âš ï¸ [{table_name}] è·³è¿‡å¤±è´¥æ‰¹æ¬¡ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹æ¬¡")
                                    break
                                
                    except Exception as e:
                        if attempt < max_retries - 1:
                            self.logger.warning(f"âš ï¸ [{table_name}] æ‰¹æ¬¡ {batch_num} è¯·æ±‚å¼‚å¸¸: {e}ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                            time.sleep(retry_delay)
                        else:
                            self.logger.error(f"âŒ [{table_name}] æ‰¹æ¬¡ {batch_num} è¯·æ±‚å¤±è´¥: {e}")
                            if not self.config['sync_options'].get('continue_on_error', False):
                                raise
                            else:
                                self.logger.warning(f"âš ï¸ [{table_name}] è·³è¿‡å¤±è´¥æ‰¹æ¬¡ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹æ¬¡")
                                break
                
                # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼Œé¿å…APIé™æµ
                if i + batch_size < len(records):
                    time.sleep(0.5)
        
        self.logger.info(f"ğŸ‰ [{table_name}] æ‰¹é‡æ’å…¥å®Œæˆï¼æ€»è®¡æˆåŠŸæ’å…¥ {len(all_record_ids)} æ¡è®°å½•")
        return all_record_ids
    
    def sync_data(self) -> Dict[str, Any]:
        """æ‰§è¡Œå¤šè¡¨æ ¼æ•°æ®åŒæ­¥"""
        start_time = time.time()
        
        try:
            self.logger.info("ğŸ¯ å¼€å§‹å¤šè¡¨æ ¼æ•°æ®åŒæ­¥ä»»åŠ¡...")
            
            # è·å–è®¿é—®ä»¤ç‰Œ
            self.get_access_token()
            
            # åŠ è½½æ•°æ®æº
            df = self.load_data_source()
            
            # åŒæ­¥ç»“æœç»Ÿè®¡
            sync_results = {
                "total_tables": len(self.config['tables']),
                "successful_tables": 0,
                "failed_tables": 0,
                "table_results": {}
            }
            
            # éå†æ‰€æœ‰è¡¨æ ¼é…ç½®
            for table_config in self.config['tables']:
                table_name = table_config['name']
                base_id = table_config['base_id']
                table_id = table_config['table_id']
                field_mapping = table_config['field_mapping']
                
                try:
                    self.logger.info(f"ğŸ“Š å¼€å§‹åŒæ­¥è¡¨æ ¼: {table_name}")
                    
                    # éªŒè¯å­—æ®µæ˜ å°„
                    self.validate_field_mapping(df, field_mapping)
                    
                    # å‡†å¤‡è®°å½•
                    records = self.prepare_records(df, field_mapping)
                    
                    # æ‰¹é‡æ’å…¥
                    record_ids = self.batch_insert_records(records, base_id, table_id, table_name)
                    
                    # è®°å½•ç»“æœ
                    sync_results["table_results"][table_name] = {
                        "success": True,
                        "records_inserted": len(record_ids),
                        "base_id": base_id,
                        "table_id": table_id
                    }
                    sync_results["successful_tables"] += 1
                    
                    self.logger.info(f"âœ… [{table_name}] åŒæ­¥å®Œæˆï¼Œæ’å…¥ {len(record_ids)} æ¡è®°å½•")
                    
                except Exception as e:
                    self.logger.error(f"âŒ [{table_name}] åŒæ­¥å¤±è´¥: {str(e)}")
                    sync_results["table_results"][table_name] = {
                        "success": False,
                        "error": str(e),
                        "base_id": base_id,
                        "table_id": table_id
                    }
                    sync_results["failed_tables"] += 1
                    
                    if not self.config['sync_options'].get('continue_on_error', True):
                        raise
            
            # è®¡ç®—æ€»è€—æ—¶
            end_time = time.time()
            duration = end_time - start_time
            
            # æ±‡æ€»ç»“æœ
            total_inserted = sum(
                result.get("records_inserted", 0) 
                for result in sync_results["table_results"].values() 
                if result.get("success", False)
            )
            
            sync_results.update({
                "duration": duration,
                "total_records_inserted": total_inserted
            })
            
            self.logger.info("ğŸ‰ å¤šè¡¨æ ¼æ•°æ®åŒæ­¥ä»»åŠ¡å®Œæˆï¼")
            self.logger.info(f"ğŸ“Š æˆåŠŸè¡¨æ ¼æ•°: {sync_results['successful_tables']}/{sync_results['total_tables']}")
            self.logger.info(f"âœ… æ€»æ’å…¥è®°å½•æ•°: {total_inserted}")
            self.logger.info(f"â±ï¸ æ€»è€—æ—¶: {duration:.2f} ç§’")
            
            return sync_results
            
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            
            self.logger.error(f"âŒ å¤šè¡¨æ ¼æ•°æ®åŒæ­¥ä»»åŠ¡å¤±è´¥: {str(e)}")
            
            return {
                "success": False,
                "error": str(e),
                "duration": duration
            }


def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºåŒæ­¥å™¨å®ä¾‹
        sync = FeishuMultiTableSync()
        
        # æ‰§è¡ŒåŒæ­¥
        result = sync.sync_data()
        
        # è¾“å‡ºç»“æœ
        if result.get("success", True):  # é»˜è®¤ä¸ºTrueï¼Œé™¤éæ˜ç¡®è®¾ç½®ä¸ºFalse
            print(f"\nğŸ‰ å¤šè¡¨æ ¼åŒæ­¥æˆåŠŸå®Œæˆï¼")
            print(f"ğŸ“Š æˆåŠŸè¡¨æ ¼æ•°: {result.get('successful_tables', 0)}/{result.get('total_tables', 0)}")
            print(f"ğŸ“ æ€»æ’å…¥è®°å½•æ•°: {result.get('total_records_inserted', 0)}")
            print(f"â±ï¸ æ€»è€—æ—¶: {result.get('duration', 0):.2f} ç§’")
            
            # æ˜¾ç¤ºå„è¡¨æ ¼è¯¦ç»†ç»“æœ
            for table_name, table_result in result.get("table_results", {}).items():
                if table_result.get("success"):
                    print(f"  âœ… {table_name}: {table_result.get('records_inserted', 0)} æ¡è®°å½•")
                else:
                    print(f"  âŒ {table_name}: {table_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        else:
            print(f"\nâŒ å¤šè¡¨æ ¼åŒæ­¥å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")


if __name__ == "__main__":
    main()