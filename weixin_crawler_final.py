#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çˆ¬è™« - æœ€ç»ˆç‰ˆæœ¬

åŠŸèƒ½ç‰¹ç‚¹:
1. æ”¯æŒåŠ¨æ€å†…å®¹åŠ è½½
2. å¤šç§å†…å®¹æå–ç­–ç•¥
3. åçˆ¬è™«æ£€æµ‹
4. è¯¦ç»†çš„é”™è¯¯å¤„ç†å’Œè°ƒè¯•ä¿¡æ¯
5. æ”¯æŒå¤šç§å¾®ä¿¡æ–‡ç« æ ¼å¼

ä½¿ç”¨è¯´æ˜:
1. ç¡®ä¿å·²å®‰è£… Chrome æµè§ˆå™¨
2. å®‰è£…ä¾èµ–: pip install selenium beautifulsoup4 lxml pandas
3. è¿è¡Œè„šæœ¬å¹¶æä¾›å¾®ä¿¡æ–‡ç« URL

æ³¨æ„äº‹é¡¹:
- æŸäº›æ–‡ç« å¯èƒ½éœ€è¦ç™»å½•å¾®ä¿¡æ‰èƒ½æŸ¥çœ‹å®Œæ•´å†…å®¹
- éƒ¨åˆ†æ–‡ç« å†…å®¹å¯èƒ½ä¸»è¦ä¸ºå›¾ç‰‡ï¼Œæ–‡æœ¬å†…å®¹è¾ƒå°‘
- å»ºè®®ä½¿ç”¨å…¬å¼€å¯è®¿é—®çš„å¾®ä¿¡æ–‡ç« è¿›è¡Œæµ‹è¯•
"""

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import pandas as pd
import re

class WeixinCrawler:
    def __init__(self, headless=True, wait_time=20):
        self.headless = headless
        self.wait_time = wait_time
        self.driver = None
    
    def setup_driver(self):
        """é…ç½®Chromeæµè§ˆå™¨"""
        chrome_options = Options()
        
        if self.headless:
            chrome_options.add_argument("--headless=new")
        
        # åæ£€æµ‹é…ç½®
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        self.driver = webdriver.Chrome(options=chrome_options)
        
        # æ‰§è¡Œåæ£€æµ‹è„šæœ¬
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    def crawl_article(self, url):
        """çˆ¬å–å¾®ä¿¡æ–‡ç« """
        if not self.driver:
            self.setup_driver()
        
        try:
            print(f"æ­£åœ¨è®¿é—®: {url}")
            self.driver.get(url)
            
            # æ£€æŸ¥é¡µé¢çŠ¶æ€
            self._check_page_status()
            
            # ç­‰å¾…å†…å®¹åŠ è½½
            self._wait_for_content()
            
            # è§¦å‘åŠ¨æ€å†…å®¹åŠ è½½
            self._trigger_content_loading()
            
            # è·å–é¡µé¢æºä»£ç 
            page_source = self.driver.page_source
            print(f"é¡µé¢æºä»£ç é•¿åº¦: {len(page_source)}")
            
            return page_source
            
        except Exception as e:
            print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    def _check_page_status(self):
        """æ£€æŸ¥é¡µé¢çŠ¶æ€"""
        page_source = self.driver.page_source
        
        if "å‚æ•°é”™è¯¯" in page_source:
            print("âš ï¸  æ£€æµ‹åˆ°å‚æ•°é”™è¯¯ï¼ŒURLå¯èƒ½æ— æ•ˆ")
        elif "éªŒè¯" in page_source or "ç™»å½•" in page_source:
            print("âš ï¸  æ£€æµ‹åˆ°éœ€è¦éªŒè¯æˆ–ç™»å½•")
        elif "è®¿é—®è¿‡äºé¢‘ç¹" in page_source:
            print("âš ï¸  è®¿é—®é¢‘ç‡è¿‡é«˜ï¼Œå»ºè®®ç¨åé‡è¯•")
        else:
            print("âœ… é¡µé¢çŠ¶æ€æ­£å¸¸")
    
    def _wait_for_content(self):
        """ç­‰å¾…å†…å®¹åŠ è½½"""
        wait = WebDriverWait(self.driver, self.wait_time)
        
        # ç­‰å¾…æ ‡é¢˜
        try:
            wait.until(EC.any_of(
                EC.presence_of_element_located((By.CSS_SELECTOR, "h1.rich_media_title")),
                EC.presence_of_element_located((By.CSS_SELECTOR, "h2.rich_media_title")),
                EC.presence_of_element_located((By.CSS_SELECTOR, ".rich_media_title")),
                EC.presence_of_element_located((By.TAG_NAME, "h1")),
                EC.presence_of_element_located((By.TAG_NAME, "h2"))
            ))
            print("âœ… æ ‡é¢˜å·²åŠ è½½")
        except:
            print("âš ï¸  æ ‡é¢˜åŠ è½½è¶…æ—¶")
        
        # ç­‰å¾…å†…å®¹åŒºåŸŸ
        try:
            wait.until(EC.any_of(
                EC.presence_of_element_located((By.ID, "js_content")),
                EC.presence_of_element_located((By.CSS_SELECTOR, ".rich_media_content")),
                EC.presence_of_element_located((By.TAG_NAME, "article"))
            ))
            print("âœ… å†…å®¹åŒºåŸŸå·²åŠ è½½")
        except:
            print("âš ï¸  å†…å®¹åŒºåŸŸåŠ è½½è¶…æ—¶")
    
    def _trigger_content_loading(self):
        """è§¦å‘åŠ¨æ€å†…å®¹åŠ è½½"""
        try:
            # æ»šåŠ¨é¡µé¢
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(1)
            
            # è§¦å‘äº‹ä»¶
            self.driver.execute_script("""
                window.dispatchEvent(new Event('scroll'));
                window.dispatchEvent(new Event('resize'));
                window.dispatchEvent(new Event('load'));
            """)
            
            # é¢å¤–ç­‰å¾…
            time.sleep(3)
            
        except Exception as e:
            print(f"è§¦å‘å†…å®¹åŠ è½½æ—¶å‡ºé”™: {e}")
    
    def extract_content(self, page_source):
        """æå–æ–‡ç« å†…å®¹"""
        soup = BeautifulSoup(page_source, "lxml")
        
        # æå–æ ‡é¢˜
        title = self._extract_title(soup)
        
        # æå–æ­£æ–‡
        content = self._extract_main_content(soup)
        
        # æå–å›¾ç‰‡ä¿¡æ¯
        images = self._extract_images(soup)
        
        # æå–è¡¨æ ¼
        tables = self._extract_tables(soup)
        
        return {
            "title": title,
            "content": content,
            "images": images,
            "tables": tables,
            "content_length": len(content),
            "image_count": len(images),
            "table_count": len(tables)
        }
    
    def _extract_title(self, soup):
        """æå–æ ‡é¢˜"""
        title_selectors = [
            "h1.rich_media_title",
            "h2.rich_media_title",
            ".rich_media_title",
            "h1",
            "h2",
            "title"
        ]
        
        for selector in title_selectors:
            elem = soup.select_one(selector)
            if elem and elem.get_text(strip=True):
                title = elem.get_text(strip=True)
                # è¿‡æ»¤æ‰é€šç”¨æ ‡é¢˜
                if title not in ["å¾®ä¿¡å…¬ä¼—å¹³å°", "å¾®ä¿¡", "WeChat"]:
                    print(f"âœ… æ‰¾åˆ°æ ‡é¢˜: {title}")
                    return title
        
        print("âš ï¸  æœªæ‰¾åˆ°æœ‰æ•ˆæ ‡é¢˜")
        return "æœªçŸ¥æ ‡é¢˜"
    
    def _extract_main_content(self, soup):
        """æå–æ­£æ–‡å†…å®¹"""
        content_selectors = [
            "#js_content",
            ".rich_media_content",
            "article",
            ".article-content",
            "main",
            ".content"
        ]
        
        for selector in content_selectors:
            elem = soup.select_one(selector)
            if elem:
                content = self._extract_text_from_element(elem)
                if content and len(content) > 50:  # è‡³å°‘50ä¸ªå­—ç¬¦æ‰è®¤ä¸ºæ˜¯æœ‰æ•ˆå†…å®¹
                    print(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨ {selector} æå–åˆ°å†…å®¹ï¼Œé•¿åº¦: {len(content)}")
                    return content
        
        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»bodyæå–
        body = soup.find("body")
        if body:
            content = self._extract_text_from_element(body)
            if content:
                print(f"âš ï¸  ä»bodyæå–å†…å®¹ï¼Œé•¿åº¦: {len(content)}")
                return content
        
        print("âŒ æœªèƒ½æå–åˆ°æœ‰æ•ˆå†…å®¹")
        return ""
    
    def _extract_text_from_element(self, element):
        """ä»å…ƒç´ ä¸­æå–æ–‡æœ¬"""
        paragraphs = []
        
        # æ–¹æ³•1: æå–æ®µè½å’Œæ–‡æœ¬å…ƒç´ 
        text_elements = element.find_all(["p", "div", "span", "section", "h1", "h2", "h3", "h4", "h5", "h6"])
        for elem in text_elements:
            text = elem.get_text(strip=True)
            if text and len(text) > 10 and text not in paragraphs:
                # è¿‡æ»¤æ‰æ˜æ˜¾çš„å¯¼èˆªæˆ–æ— å…³å†…å®¹
                if not self._is_navigation_text(text):
                    paragraphs.append(text)
        
        # æ–¹æ³•2: å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ®µè½ï¼Œæå–æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
        if not paragraphs:
            for string in element.stripped_strings:
                text = string.strip()
                if len(text) > 10 and not self._is_navigation_text(text):
                    paragraphs.append(text)
        
        return "\n\n".join(paragraphs) if paragraphs else ""
    
    def _is_navigation_text(self, text):
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¯¼èˆªæˆ–æ— å…³æ–‡æœ¬"""
        nav_keywords = [
            "å¾®ä¿¡æ‰«ä¸€æ‰«", "å…³æ³¨è¯¥å…¬ä¼—å·", "åˆ†äº«åˆ°", "ç‚¹å‡»æŸ¥çœ‹", "é˜…è¯»åŸæ–‡",
            "åœ¨çœ‹", "ç‚¹èµ", "åˆ†äº«", "æ”¶è—", "å†™ç•™è¨€", "è§†é¢‘å°ç¨‹åº",
            "è½»ç‚¹ä¸¤ä¸‹", "å–æ¶ˆèµ", "å–æ¶ˆåœ¨çœ‹", "å‚æ•°é”™è¯¯"
        ]
        
        return any(keyword in text for keyword in nav_keywords)
    
    def _extract_images(self, soup):
        """æå–å›¾ç‰‡ä¿¡æ¯"""
        images = []
        img_elements = soup.find_all("img")
        
        for img in img_elements:
            img_info = {
                "src": img.get("src", ""),
                "data-src": img.get("data-src", ""),
                "alt": img.get("alt", ""),
                "title": img.get("title", "")
            }
            if img_info["src"] or img_info["data-src"]:
                images.append(img_info)
        
        return images
    
    def _extract_tables(self, soup):
        """æå–è¡¨æ ¼æ•°æ®"""
        tables = []
        table_elements = soup.find_all("table")
        
        for table in table_elements:
            try:
                df = pd.read_html(str(table))[0]
                tables.append(df)
            except Exception as e:
                print(f"è¡¨æ ¼è§£æå¤±è´¥: {e}")
        
        return tables
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            self.driver.quit()
            self.driver = None
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç”¨æ³•"""
    # æµ‹è¯•URLåˆ—è¡¨
    test_urls = [
        "https://mp.weixin.qq.com/s/lt-up7a7BeYY5nVUUOsCeQ",
        # å¯ä»¥æ·»åŠ æ›´å¤šæµ‹è¯•URL
    ]
    
    # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºæ­£ç¡®é‡Šæ”¾
    with WeixinCrawler(headless=True, wait_time=15) as crawler:
        for i, url in enumerate(test_urls, 1):
            print(f"\n{'='*80}")
            print(f"æµ‹è¯• {i}: å¾®ä¿¡æ–‡ç« çˆ¬å–")
            print(f"URL: {url}")
            print(f"{'='*80}")
            
            try:
                # çˆ¬å–é¡µé¢
                page_source = crawler.crawl_article(url)
                
                # æå–å†…å®¹
                result = crawler.extract_content(page_source)
                
                # è¾“å‡ºç»“æœ
                print(f"\nğŸ“Š æå–ç»“æœ:")
                print(f"æ ‡é¢˜: {result['title']}")
                print(f"å†…å®¹é•¿åº¦: {result['content_length']} å­—ç¬¦")
                print(f"å›¾ç‰‡æ•°é‡: {result['image_count']}")
                print(f"è¡¨æ ¼æ•°é‡: {result['table_count']}")
                
                if result['content']:
                    print(f"\nğŸ“ å†…å®¹é¢„è§ˆ:")
                    preview = result['content'][:300] + "..." if len(result['content']) > 300 else result['content']
                    print(preview)
                else:
                    print(f"\nâš ï¸  æœªèƒ½æå–åˆ°æ–‡æœ¬å†…å®¹")
                    print(f"å¯èƒ½çš„åŸå› :")
                    print(f"- æ–‡ç« ä¸»è¦åŒ…å«å›¾ç‰‡å†…å®¹")
                    print(f"- éœ€è¦ç™»å½•å¾®ä¿¡æŸ¥çœ‹")
                    print(f"- URLå·²å¤±æ•ˆæˆ–éœ€è¦ç‰¹æ®Šæƒé™")
                    print(f"- å†…å®¹å®Œå…¨ç”±JavaScriptåŠ¨æ€ç”Ÿæˆ")
                
                if result['images']:
                    print(f"\nğŸ–¼ï¸  å›¾ç‰‡ä¿¡æ¯:")
                    for j, img in enumerate(result['images'][:3], 1):  # åªæ˜¾ç¤ºå‰3å¼ 
                        print(f"å›¾ç‰‡ {j}: {img['alt'] or 'æ— æè¿°'}")
                
                if result['tables']:
                    print(f"\nğŸ“‹ è¡¨æ ¼ä¿¡æ¯:")
                    for j, table in enumerate(result['tables'], 1):
                        print(f"è¡¨æ ¼ {j}: {table.shape[0]}è¡Œ x {table.shape[1]}åˆ—")
                
            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆ")
    print(f"\nğŸ’¡ ä½¿ç”¨æç¤º:")
    print(f"1. å¦‚æœå†…å®¹æå–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å…¬å¼€å¯è®¿é—®çš„å¾®ä¿¡æ–‡ç« ")
    print(f"2. æŸäº›æ–‡ç« å¯èƒ½éœ€è¦åœ¨å¾®ä¿¡ä¸­æ‰“å¼€æ‰èƒ½æŸ¥çœ‹å®Œæ•´å†…å®¹")
    print(f"3. å¯ä»¥ä¿®æ”¹ headless=False æ¥è§‚å¯Ÿæµè§ˆå™¨è¡Œä¸º")
    print(f"4. è°ƒæ•´ wait_time å‚æ•°æ¥é€‚åº”ä¸åŒçš„ç½‘ç»œç¯å¢ƒ")

if __name__ == "__main__":
    main()