#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çˆ¬è™« - å¸¦OCRå›¾ç‰‡æ–‡å­—è¯†åˆ«åŠŸèƒ½
æ”¯æŒæå–å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹

ä¾èµ–å®‰è£…:
pip install selenium beautifulsoup4 lxml pandas pillow pytesseract easyocr

æ³¨æ„: éœ€è¦å®‰è£…Tesseract OCRå¼•æ“
macOS: brew install tesseract tesseract-lang
Ubuntu: sudo apt-get install tesseract-ocr tesseract-ocr-chi-sim
Windows: ä¸‹è½½å®‰è£…åŒ… https://github.com/UB-Mannheim/tesseract/wiki
"""

import time
import requests
import traceback
from io import BytesIO
from urllib.parse import urljoin, urlparse
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
from PIL import Image
import pandas as pd

# OCRå¼•æ“é€‰æ‹©
try:
    import pytesseract
    TESSERACT_AVAILABLE = True
except ImportError:
    TESSERACT_AVAILABLE = False
    print("âš ï¸  Tesseract OCRæœªå®‰è£…ï¼Œå°†è·³è¿‡OCRåŠŸèƒ½")

try:
    import easyocr
    EASYOCR_AVAILABLE = True
except ImportError:
    EASYOCR_AVAILABLE = False
    print("âš ï¸  EasyOCRæœªå®‰è£…ï¼Œå°†è·³è¿‡EasyOCRåŠŸèƒ½")


class WeixinCrawlerWithOCR:
    """å¾®ä¿¡æ–‡ç« çˆ¬è™« - å¸¦OCRåŠŸèƒ½"""
    
    def __init__(self, headless=True, wait_time=15, ocr_engine='auto'):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            wait_time: é¡µé¢åŠ è½½ç­‰å¾…æ—¶é—´
            ocr_engine: OCRå¼•æ“é€‰æ‹© ('tesseract', 'easyocr', 'auto')
        """
        self.headless = headless
        self.wait_time = wait_time
        self.driver = None
        self.ocr_engine = ocr_engine
        self.easyocr_reader = None
        
        # åˆå§‹åŒ–OCRå¼•æ“
        self._init_ocr()
    
    def _init_ocr(self):
        """åˆå§‹åŒ–OCRå¼•æ“"""
        if self.ocr_engine == 'auto':
            if EASYOCR_AVAILABLE:
                self.ocr_engine = 'easyocr'
                print("ğŸ” ä½¿ç”¨EasyOCRå¼•æ“")
            elif TESSERACT_AVAILABLE:
                self.ocr_engine = 'tesseract'
                print("ğŸ” ä½¿ç”¨Tesseract OCRå¼•æ“")
            else:
                self.ocr_engine = None
                print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„OCRå¼•æ“")
        
        if self.ocr_engine == 'easyocr' and EASYOCR_AVAILABLE:
            try:
                self.easyocr_reader = easyocr.Reader(['ch_sim', 'en'])
                print("âœ… EasyOCRåˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âŒ EasyOCRåˆå§‹åŒ–å¤±è´¥: {e}")
                self.ocr_engine = 'tesseract' if TESSERACT_AVAILABLE else None
    
    def setup_driver(self):
        """è®¾ç½®Chromeæµè§ˆå™¨é©±åŠ¨"""
        chrome_options = Options()
        
        if self.headless:
            chrome_options.add_argument("--headless=new")
        
        # åçˆ¬è™«æ£€æµ‹é…ç½®
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # è®¾ç½®User-Agent
        chrome_options.add_argument(
            "--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        
        self.driver = webdriver.Chrome(options=chrome_options)
        
        # æ‰§è¡Œåæ£€æµ‹è„šæœ¬
        self.driver.execute_script(
            "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
        )
        
        print("âœ… æµè§ˆå™¨é©±åŠ¨åˆå§‹åŒ–å®Œæˆ")
    
    def crawl_article(self, url):
        """çˆ¬å–å¾®ä¿¡æ–‡ç« é¡µé¢æºä»£ç """
        if not self.driver:
            self.setup_driver()
        
        try:
            print(f"æ­£åœ¨è®¿é—®: {url}")
            self.driver.get(url)
            
            # æ£€æŸ¥é¡µé¢çŠ¶æ€
            if "å‚æ•°é”™è¯¯" in self.driver.page_source or "è¯¥å†…å®¹å·²è¢«å‘å¸ƒè€…åˆ é™¤" in self.driver.page_source:
                print("âš ï¸  æ£€æµ‹åˆ°å‚æ•°é”™è¯¯ï¼ŒURLå¯èƒ½æ— æ•ˆ")
            
            # ç­‰å¾…é¡µé¢åŸºæœ¬åŠ è½½
            time.sleep(3)
            
            # ç­‰å¾…æ ‡é¢˜åŠ è½½
            wait = WebDriverWait(self.driver, self.wait_time)
            try:
                wait.until(EC.any_of(
                    EC.presence_of_element_located((By.ID, "activity-name")),
                    EC.presence_of_element_located((By.CLASS_NAME, "rich_media_title")),
                    EC.presence_of_element_located((By.TAG_NAME, "h1"))
                ))
                print("âœ… æ ‡é¢˜å·²åŠ è½½")
            except TimeoutException:
                print("âš ï¸  æ ‡é¢˜åŠ è½½è¶…æ—¶")
            
            # ç­‰å¾…å†…å®¹åŒºåŸŸåŠ è½½
            try:
                wait.until(EC.any_of(
                    EC.presence_of_element_located((By.ID, "js_content")),
                    EC.presence_of_element_located((By.CLASS_NAME, "rich_media_content")),
                    EC.presence_of_element_located((By.TAG_NAME, "article"))
                ))
                print("âœ… å†…å®¹åŒºåŸŸå·²åŠ è½½")
            except TimeoutException:
                print("âš ï¸  å†…å®¹åŒºåŸŸåŠ è½½è¶…æ—¶")
            
            # æ»šåŠ¨é¡µé¢è§¦å‘æ‡’åŠ è½½
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(1)
            
            # è§¦å‘å¯èƒ½çš„åŠ¨æ€å†…å®¹åŠ è½½
            self.driver.execute_script("""
                // è§¦å‘å„ç§å¯èƒ½çš„äº‹ä»¶
                window.dispatchEvent(new Event('scroll'));
                window.dispatchEvent(new Event('resize'));
                document.dispatchEvent(new Event('DOMContentLoaded'));
            """)
            
            time.sleep(3)
            
            page_source = self.driver.page_source
            print(f"é¡µé¢æºä»£ç é•¿åº¦: {len(page_source)}")
            
            return page_source
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
            traceback.print_exc()
            return None
    
    def download_image(self, img_url, base_url):
        """ä¸‹è½½å›¾ç‰‡"""
        try:
            # å¤„ç†ç›¸å¯¹URL
            if img_url.startswith('//'):
                img_url = 'https:' + img_url
            elif img_url.startswith('/'):
                img_url = urljoin(base_url, img_url)
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Referer': base_url
            }
            
            response = requests.get(img_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            # è½¬æ¢ä¸ºPIL Imageå¯¹è±¡
            image = Image.open(BytesIO(response.content))
            return image
            
        except Exception as e:
            print(f"âš ï¸  å›¾ç‰‡ä¸‹è½½å¤±è´¥ {img_url}: {e}")
            return None
    
    def extract_text_from_image(self, image):
        """ä»å›¾ç‰‡ä¸­æå–æ–‡å­—"""
        if not image or not self.ocr_engine:
            return ""
        
        try:
            if self.ocr_engine == 'tesseract' and TESSERACT_AVAILABLE:
                # ä½¿ç”¨Tesseract OCR
                text = pytesseract.image_to_string(image, lang='chi_sim+eng')
                return text.strip()
            
            elif self.ocr_engine == 'easyocr' and self.easyocr_reader:
                # ä½¿ç”¨EasyOCR
                import numpy as np
                img_array = np.array(image)
                results = self.easyocr_reader.readtext(img_array)
                
                # æå–æ–‡å­—å†…å®¹
                texts = []
                for (bbox, text, confidence) in results:
                    if confidence > 0.5:  # ç½®ä¿¡åº¦é˜ˆå€¼
                        texts.append(text)
                
                return ' '.join(texts)
            
        except Exception as e:
            print(f"âš ï¸  OCRè¯†åˆ«å¤±è´¥: {e}")
        
        return ""
    
    def extract_content(self, page_source, base_url=None):
        """æå–æ–‡ç« å†…å®¹ï¼ŒåŒ…æ‹¬OCRå›¾ç‰‡æ–‡å­—è¯†åˆ«"""
        if not page_source:
            return {
                'title': 'æœªçŸ¥æ ‡é¢˜',
                'content': '',
                'images': [],
                'image_texts': [],  # æ–°å¢ï¼šå›¾ç‰‡ä¸­çš„æ–‡å­—
                'tables': []
            }
        
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = "æœªçŸ¥æ ‡é¢˜"
        title_selectors = [
            "#activity-name",
            ".rich_media_title",
            "h1",
            ".title",
            "title"
        ]
        
        for selector in title_selectors:
            title_element = soup.select_one(selector)
            if title_element and title_element.get_text(strip=True):
                title = title_element.get_text(strip=True)
                print(f"âœ… æ‰¾åˆ°æ ‡é¢˜: {title}")
                break
        
        if title == "æœªçŸ¥æ ‡é¢˜":
            print("âš ï¸  æœªæ‰¾åˆ°æœ‰æ•ˆæ ‡é¢˜")
        
        # æå–å†…å®¹
        content_selectors = [
            "#js_content",
            ".rich_media_content",
            "article",
            ".article-content",
            "main",
            ".content"
        ]
        
        content_element = None
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                content_element = element
                print(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨ {selector} æå–åˆ°å†…å®¹ï¼Œé•¿åº¦: {len(element.get_text(strip=True))}")
                break
        
        if not content_element:
            print("âš ï¸  æœªæ‰¾åˆ°å†…å®¹åŒºåŸŸï¼Œå°è¯•ä»bodyæå–")
            content_element = soup.find('body')
        
        # æå–æ–‡æœ¬å†…å®¹
        paragraphs = []
        images = []
        image_texts = []  # å­˜å‚¨å›¾ç‰‡ä¸­è¯†åˆ«çš„æ–‡å­—
        
        if content_element:
            # æå–å›¾ç‰‡ä¿¡æ¯å’ŒOCRæ–‡å­—
            img_elements = content_element.find_all('img')
            print(f"ğŸ–¼ï¸  æ‰¾åˆ° {len(img_elements)} å¼ å›¾ç‰‡")
            
            for i, img in enumerate(img_elements, 1):
                img_info = {
                    'src': img.get('src', ''),
                    'alt': img.get('alt', ''),
                    'title': img.get('title', '')
                }
                images.append(img_info)
                
                # OCRè¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—
                if self.ocr_engine and img_info['src']:
                    print(f"ğŸ” æ­£åœ¨è¯†åˆ«ç¬¬ {i} å¼ å›¾ç‰‡ä¸­çš„æ–‡å­—...")
                    image_obj = self.download_image(img_info['src'], base_url or '')
                    if image_obj:
                        ocr_text = self.extract_text_from_image(image_obj)
                        if ocr_text:
                            image_texts.append({
                                'image_index': i,
                                'text': ocr_text,
                                'src': img_info['src']
                            })
                            print(f"âœ… å›¾ç‰‡ {i} è¯†åˆ«åˆ°æ–‡å­—: {ocr_text[:50]}...")
                            
                            # å°†OCRè¯†åˆ«çš„æ–‡å­—æ·»åŠ åˆ°æ­£æ–‡å†…å®¹ä¸­
                            paragraphs.append(f"[å›¾ç‰‡{i}æ–‡å­—å†…å®¹]: {ocr_text}")
                        else:
                            print(f"âš ï¸  å›¾ç‰‡ {i} æœªè¯†åˆ«åˆ°æ–‡å­—")
            
            # æå–å¸¸è§„æ–‡æœ¬å†…å®¹
            text_elements = content_element.find_all(['p', 'div', 'span', 'section', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            
            for element in text_elements:
                text = element.get_text(strip=True)
                if text and len(text) > 10 and not self._is_navigation_text(text):
                    paragraphs.append(text)
            
            # å¦‚æœæ²¡æœ‰æå–åˆ°è¶³å¤Ÿçš„æ–‡æœ¬ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            if len(paragraphs) < 3:
                for string in content_element.stripped_strings:
                    text = string.strip()
                    if len(text) > 10 and not self._is_navigation_text(text):
                        paragraphs.append(text)
        
        # æå–è¡¨æ ¼
        tables = []
        table_elements = soup.find_all('table')
        for table in table_elements:
            table_data = []
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                row_data = [cell.get_text(strip=True) for cell in cells]
                if any(row_data):  # åªæ·»åŠ éç©ºè¡Œ
                    table_data.append(row_data)
            if table_data:
                tables.append(table_data)
        
        # å»é‡å’Œè¿‡æ»¤
        unique_paragraphs = []
        seen = set()
        for p in paragraphs:
            if p not in seen and len(p.strip()) > 5:
                unique_paragraphs.append(p)
                seen.add(p)
        
        content = '\n\n'.join(unique_paragraphs)
        
        return {
            'title': title,
            'content': content,
            'images': images,
            'image_texts': image_texts,  # æ–°å¢ï¼šå›¾ç‰‡ä¸­çš„æ–‡å­—
            'tables': tables
        }
    
    def _is_navigation_text(self, text):
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¯¼èˆªæˆ–æ— å…³æ–‡æœ¬"""
        nav_keywords = [
            "å¾®ä¿¡æ‰«ä¸€æ‰«", "å…³æ³¨è¯¥å…¬ä¼—å·", "åˆ†äº«åˆ°", "ç‚¹å‡»æŸ¥çœ‹", "é˜…è¯»åŸæ–‡",
            "åœ¨çœ‹", "ç‚¹èµ", "åˆ†äº«", "æ”¶è—", "å†™ç•™è¨€", "æœ‹å‹åœˆ", "QQç©ºé—´",
            "æ–°æµªå¾®åš", "QQå¥½å‹", "è¯·åœ¨å¾®ä¿¡æœç´¢", "é•¿æŒ‰è¯†åˆ«", "æ‰«æäºŒç»´ç "
        ]
        return any(keyword in text for keyword in nav_keywords)
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            self.driver.quit()
            print("âœ… æµè§ˆå™¨å·²å…³é—­")
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.close()


def test_weixin_crawler_with_ocr():
    """æµ‹è¯•å¾®ä¿¡çˆ¬è™«OCRåŠŸèƒ½"""
    # æµ‹è¯•URLåˆ—è¡¨
    test_urls = [
        "https://mp.weixin.qq.com/s/lt-up7a7BeYY5nVUU0sCeQ",  # æ™®ç¿å¸æ›¼æ ¡å›­æ‹›è˜
        "https://mp.weixin.qq.com/s/Ub9BKKto3kSCH5tXXE_Ztg",  # å¤‡ç”¨æµ‹è¯•URL
    ]
    
    print("ğŸš€ å¾®ä¿¡æ–‡ç« çˆ¬è™« - OCRåŠŸèƒ½æµ‹è¯•")
    print("=" * 80)
    
    for i, url in enumerate(test_urls, 1):
        print(f"\næµ‹è¯• {i}: å¾®ä¿¡æ–‡ç« çˆ¬å– + OCRæ–‡å­—è¯†åˆ«")
        print(f"URL: {url}")
        print("=" * 80)
        
        try:
            with WeixinCrawlerWithOCR(headless=True, ocr_engine='auto') as crawler:
                # çˆ¬å–é¡µé¢
                page_source = crawler.crawl_article(url)
                
                if page_source:
                    # æå–å†…å®¹
                    result = crawler.extract_content(page_source, url)
                    
                    print(f"\nğŸ“Š æå–ç»“æœ:")
                    print(f"æ ‡é¢˜: {result['title']}")
                    print(f"å†…å®¹é•¿åº¦: {len(result['content'])} å­—ç¬¦")
                    print(f"å›¾ç‰‡æ•°é‡: {len(result['images'])}")
                    print(f"è¯†åˆ«åˆ°æ–‡å­—çš„å›¾ç‰‡æ•°é‡: {len(result['image_texts'])}")
                    print(f"è¡¨æ ¼æ•°é‡: {len(result['tables'])}")
                    
                    # æ˜¾ç¤ºå›¾ç‰‡OCRç»“æœ
                    if result['image_texts']:
                        print(f"\nğŸ” å›¾ç‰‡æ–‡å­—è¯†åˆ«ç»“æœ:")
                        for img_text in result['image_texts']:
                            print(f"å›¾ç‰‡ {img_text['image_index']}: {img_text['text'][:100]}...")
                    
                    # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
                    if result['content']:
                        print(f"\nğŸ“„ å†…å®¹é¢„è§ˆ:")
                        preview = result['content'][:200] + "..." if len(result['content']) > 200 else result['content']
                        print(preview)
                    else:
                        print(f"\nâš ï¸  æœªèƒ½æå–åˆ°æ–‡æœ¬å†…å®¹")
                        print("å¯èƒ½çš„åŸå› :")
                        print("- æ–‡ç« ä¸»è¦åŒ…å«å›¾ç‰‡å†…å®¹ï¼ˆå·²é€šè¿‡OCRè¯†åˆ«ï¼‰")
                        print("- éœ€è¦ç™»å½•å¾®ä¿¡æŸ¥çœ‹")
                        print("- URLå·²å¤±æ•ˆæˆ–éœ€è¦ç‰¹æ®Šæƒé™")
                        print("- å†…å®¹å®Œå…¨ç”±JavaScriptåŠ¨æ€ç”Ÿæˆ")
                else:
                    print("âŒ æœªèƒ½è·å–é¡µé¢å†…å®¹")
                    
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
        
        print("âœ… æµ‹è¯•å®Œæˆ")
    
    print("\nğŸ’¡ OCRåŠŸèƒ½ä½¿ç”¨æç¤º:")
    print("1. é¦–æ¬¡ä½¿ç”¨EasyOCRä¼šä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Œè¯·è€å¿ƒç­‰å¾…")
    print("2. OCRè¯†åˆ«å‡†ç¡®ç‡å–å†³äºå›¾ç‰‡è´¨é‡å’Œæ–‡å­—æ¸…æ™°åº¦")
    print("3. æ”¯æŒä¸­è‹±æ–‡æ··åˆè¯†åˆ«")
    print("4. å¯ä»¥é€šè¿‡è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼æ¥è¿‡æ»¤ä½è´¨é‡è¯†åˆ«ç»“æœ")
    print("5. å»ºè®®åœ¨ç½‘ç»œè‰¯å¥½çš„ç¯å¢ƒä¸‹ä½¿ç”¨ï¼Œå›¾ç‰‡ä¸‹è½½éœ€è¦æ—¶é—´")


if __name__ == "__main__":
    test_weixin_crawler_with_ocr()